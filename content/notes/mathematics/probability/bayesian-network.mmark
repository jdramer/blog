---
title: "Bayesian Network"
date: 2020-03-05 18:11:00
---

$$
\newcommand\ci{\perp\!\!\!\perp}
\newcommand\given[1][]{\:#1\vert\:}
$$

## Introduction

A Bayesian network is a directed graph in which each node is annotated with quantitative probability information. The full specification is as follows:

1. Each node corresponds to a random variable, which may be discrete or continuous.
2. A set of directed links or arrows connects pairs of nodes. If there is an arrow from node $$X$$ to node $$Y$$, $$X$$ is said to be a parent of $$Y$$. The graph has no directed cycles (and hence is a directed acyclic graph, or DAG.
3. Each node $$X_i$$ has a conditional probability distribution $$P(X_i|Parents(X_i))$$ that quantifies the effect of the parents on the node.

{{< figure src="/images/bayesian_network_full.jpg" title="Example Bayesian Network" class="is-responsive is-75p" >}}

Semantics of a bayesian network:

- The network is a representation of a joint probability distribution
- Encoding of a collection of conditional independence statements

## Full joint distribution

Given by:

$$
\begin{align}
\label{eq}
P(x_1, \ldots, x_n) = \prod_{i=1}^{N} P(x_i|Parents(X_i))
\end{align}
$$

Which can be rewritten as:

$$
\begin{align*}
P(x_1, \ldots, x_n) &= P(x_n | x_{n-1}, \ldots x_1) P(x_{n-1}, \ldots, x_1) \\
&= P(x_n | x_{n-1}, \ldots x_1) P(x_{n-1} | x_{n-2}, \ldots, x_1) \cdots P(x_2 | x_1) P(x_1) \\
&= \prod_{i=1}^{N} P(x_i | x_{i-1}, \ldots x_1) \;\; \text{(identity known as chain rule)}
\end{align*}
$$

The above is equivalent to

$$
P(X_i | X_{i-1}, \ldots X_1) = P(X_i|Parents(X_i))
$$

Provided that $$Parents(X_i) \subseteq \{ X_{i-1}, \ldots, X_1 \}$$

## Conditional independence relations in bayesian networks

Steps to determine if two variables are conditionally independent

1. **Draw the ancestral graph** Construct the "ancestral graph" of all variables mentioned in the probability expression. This is a reduced version of the original net, consisting only of the variables mentioned and all of their ancestors (parents, parents' parents, etc.)
2. **_Moralize_ the ancestral graph by _marrying_ the parents** For each pair of variables with a common child, draw an undirected edge (line) between them. (If a variable has more than two parents, draw lines between every pair of parents.)
3. **_Disorient_ the graph by replacing the directed edges (arrows) with undirected edges (lines)**.
4. **Delete the givens and their edges**. If the independence question had any given variables, erase those variables from the graph and erase all of their connections, too.
5. **Given a query between two variables A, B**
  1. If the variables are **disconnected** then they're independent
  2. If the variables are **connected** then they're dependent
  3. If the variables are **missing** because they were a given, they're independent

In the following example we skip step 1 and moralize the entire bayesian network

<div class="columns">
  <div class="column is-size-6">
    {{< figure src="/images/bayesian_network_directed.jpg" title="Example Bayesian Network" >}}
  </div>
  <div class="column is-size-6">
    {{< figure src="/images/bayesian_network_moralized.jpg" title="Example Bayesian Network Moralized" >}}
  </div>
</div>

Some conditional independence queries ($$\ci$$ meaning conditionally independent of), delete the givens and their edges to check the connection between the query variables:

- Is $$A \ci B \given C$$? **No**, there is a path A-B
- Is $$A \ci E \given C$$? **No**, there is a path A-B-D-E
- Is $$A \ci E \given C,D$$? **Yes**, there isn't a path between A and E
- Is $$A \ci D \given C$$? **No**, there is a path A-B-D
- Is $$B \ci E \given C$$? **No**, there is a path B-D-E
- Is $$A \ci F \given C$$? **No**, there is a path A-B-D-E-F
- Is $$A \ci F \given C,D$$? **Yes**, there isn't a path between A and F

## Exact inference

Compute the **posterior probablity distribution** for a set of **query values** given some observed **event (set of evidence variables)**

### By enumeration

Any conditional probability can be computed by summing terms from the full joint distribution

$$
\textbf{P}(X \given \textbf{e}) = \alpha \textbf{P} (X, e) = \alpha \sum_{y} \textbf{P} (X, \textbf{e}, \textbf{y})
$$

Working with the example below we can answer some queries:

{{< figure src="/images/bayesian_network_full.jpg" title="Example Bayesian Network" class="is-responsive is-75p" >}}

$$
\begin{align*}
P(c|a) &= \alpha \sum_{b} P(a) P(b) P(c \given a,b) \\
&= \alpha P(a) \sum_{b} P(b) P(c \given a,b) \\
&= \alpha P(a) \sum_{b} \begin{bmatrix} P(b) = 0.4 \\ P(\neg b) = 0.6 \end{bmatrix} \begin{bmatrix} P(c \given a, b) = 0.55 \\ P(c \given a, \neg b) = 0.5 \end{bmatrix} \\
&= \alpha P(a) \sum_{b} \begin{bmatrix} f(b) = 0.22 \\ f(\neg b) = 0.3 \end{bmatrix} \\
&= \alpha \begin{bmatrix} 0.2 \\ 0.8 \end{bmatrix} 0.53 \\
&= \alpha <.106, .424> \\
&= <0.2, 0.8>
\end{align*}
$$

## Reading

- https://anesi.com/bayes.htm
- http://www.cs.cmu.edu/~guestrin/Class/10701/recitations/r9/var_elim_recitation_unanimated.pdf
- https://betterexplained.com/articles/understanding-bayes-theorem-with-ratios/
- https://www.seas.upenn.edu/~cis391/Lectures/probability-bayes-2015.pdf
